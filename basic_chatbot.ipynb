{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import spacy\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Chatbots are nearly ubiquitous now, performing tasks like answering customer service questions on websites. Two basic tasks that a chatbot must perform are intent classification and named entity recognition. This can be explained by example. Suppose a user goes on a website for a clothing retailer and asks the chatbot \"Do you have any red dresses.\" First the bot must figure out what the user wants to do, what their intent is. In this case, the intent is something like ItemLookup. But what does the user want to look up? The bot must parse the text and figure that out. This is named entity recognition.\n",
    "\n",
    "In this notebook, I build a simple stateless chatbot that performs intent classification and basic named entity recognition. To classify intents, I use an LSTM. To perform named entity recognition, I use spaCy. Note that there are MANY limitations on this notebook, largely due to the small corpus. For example, I do not split the data into training and test sets and the model is unlikely to generalize well. These are known limitations, not methodological oversights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "The data is stored in JSON format. The data consists of intents along with associated input text and responses. At a high level, the LSTM will be trained on the input text to predict intents. Predicted intents will then be used to randomly select appropriate response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.load(open('intent.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine all of the intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting\n",
      "GreetingResponse\n",
      "CourtesyGreeting\n",
      "CourtesyGreetingResponse\n",
      "CurrentHumanQuery\n",
      "NameQuery\n",
      "RealNameQuery\n",
      "TimeQuery\n",
      "Thanks\n",
      "NotTalking2U\n",
      "UnderstandQuery\n",
      "Shutup\n",
      "Swearing\n",
      "GoodBye\n",
      "CourtesyGoodBye\n",
      "WhoAmI\n",
      "Clever\n",
      "Gossip\n",
      "Jokes\n",
      "PodBayDoor\n",
      "PodBayDoorResponse\n",
      "SelfAware\n"
     ]
    }
   ],
   "source": [
    "for intent in intents['intents']:\n",
    "    print(intent['intent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to know the structure of each intent in the JSON so I can iterate through it to make my inputs and targets. To do so, I examine a single intent at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'UnderstandQuery',\n",
       " 'text': ['Do you understand what I am saying',\n",
       "  'Do you understand me',\n",
       "  'Do you know what I am saying',\n",
       "  'Do you get me',\n",
       "  'Comprendo',\n",
       "  'Know what I mean'],\n",
       " 'responses': ['Well I would not be a very clever AI if I did not would I?',\n",
       "  'I read you loud and clear!',\n",
       "  'I do in deed!'],\n",
       " 'extension': {'function': '', 'entities': False, 'responses': []},\n",
       " 'context': {'in': '', 'out': '', 'clear': False},\n",
       " 'entities': []}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents['intents'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to do NER myself, so I only need intents and texts to train. I will also need responses later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "all_text = ''\n",
    "response_lookup = {}\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for input_text in intent['text']:\n",
    "        # do some preprocessing on the text\n",
    "        input_no_punct = ''.join([char.lower() for char in input_text if char not in string.punctuation]) # Remove punctuation\n",
    "        # Replace \"hi\" (first word in corpus) with UNK\n",
    "        input_with_unk = re.sub(fr'\\bhi\\b', 'UNK', input_no_punct) # I create an unknown token so the model can handle out of vocabulary (oov) words\n",
    "        input_tokens = word_tokenize(input_with_unk) # Tokenize\n",
    "\n",
    "        X.append(input_tokens)\n",
    "        y.append(intent['intent'])\n",
    "\n",
    "    response_lookup[intent['intent']] = []\n",
    "    for response_text in intent['responses']:\n",
    "        response_lookup[intent['intent']].append(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there are very few input strings. If you examine the JSON, you'll note that these are disproportionately associated with things like the Joke intent as opposed to the Greeting intent. The small number of input strings are why I do not use a train/test/validation split, despite obvious drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Text and Labels\n",
    "The LSTM will take in the input text not as text but as lists of integers. Each integer will correspond to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary maps words to integers\n",
    "input_dictionary = Dictionary(documents=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists of tokens to list of indices and pad lists\n",
    "pad_length = max([len(sent) for sent in X])\n",
    "\n",
    "X_vecs = np.zeros((len(X), pad_length)) # This effectively pads shorter sequences with 0s\n",
    "\n",
    "for i, sent in enumerate(X):\n",
    "    vectorized_sent = input_dictionary.doc2idx(sent)\n",
    "    X_vecs[i, :len(vectorized_sent)] = vectorized_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelBinarizer()\n",
    "y_enc = enc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "The model is simple. It uses trainable embeddings followed by an LSTM then two dense layers, the former of which has a ReLu activation and the latter of which uses a softmax for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "vocab_size = len(input_dictionary)\n",
    "embed_dim = 100\n",
    "units = 256\n",
    "output_size = y_enc.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 9)]               0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 9, 100)            11700     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               365568    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 22)                5654      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 448,714\n",
      "Trainable params: 448,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=X_vecs.shape[1])\n",
    "x = Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = LSTM(units)(x)\n",
    "x = Dense(units, activation='relu')(x)\n",
    "outputs = Dense(output_size, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 5s 42ms/step - loss: 3.0931 - accuracy: 0.0280\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 3.0788 - accuracy: 0.0979\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 3.0616 - accuracy: 0.1469\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 3.0153 - accuracy: 0.1888\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 2.8638 - accuracy: 0.1748\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 2.6681 - accuracy: 0.1818\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 2.4788 - accuracy: 0.2727\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 2.1542 - accuracy: 0.3007\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 1.8542 - accuracy: 0.3706\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 1.5948 - accuracy: 0.4406\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 1.4058 - accuracy: 0.5035\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 1.1650 - accuracy: 0.5874\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 1.0230 - accuracy: 0.6783\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.8423 - accuracy: 0.7413\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.7556 - accuracy: 0.7063\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.6780 - accuracy: 0.7552\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.5793 - accuracy: 0.7972\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.4401 - accuracy: 0.8392 0s - loss: 0.4401 - accuracy: 0.83\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.3163 - accuracy: 0.8811\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.2609 - accuracy: 0.9231\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.1931 - accuracy: 0.9441\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.1225 - accuracy: 0.9720\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0869 - accuracy: 0.9860\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.0614 - accuracy: 0.9930\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0020 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d03fcc910>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Adam()\n",
    "es = EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X_vecs, y_enc, epochs=epochs, callbacks=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions, NER and Chatbot\n",
    "The following functions are used to process input text, predict off of the processed input, perform NER if necessary and to actually run the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to know the vocab to know whether to replace tokens with the UNK token\n",
    "vocab = list(input_dictionary.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below processes the input text. The input text is generally processed the same way as the text used to train the model. The exception is that words that are not in the training corpus are placed with the unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = 0 # Change in future. Not ideal since 0 = 'hi' as well\n",
    "\n",
    "def process_message(message, input_dictionary=input_dictionary, pad_length=pad_length):\n",
    "    message_no_punct = ''.join([char.lower() for char in message if char not in string.punctuation]) # Remove punctuation\n",
    "    message_tokens = word_tokenize(message_no_punct) # Tokenize\n",
    "    message_unk_replace = [tok if tok in vocab else 'UNK' for tok in message_tokens] # Replace oov tokens with UNK\n",
    "\n",
    "    message_vectorized = input_dictionary.doc2idx(message_unk_replace)\n",
    "    \n",
    "    while len(message_vectorized) < pad_length:\n",
    "        message_vectorized.append(pad_token)\n",
    "\n",
    "    return np.array([message_vectorized])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I write a function that takes in a message and returns a predicted intent. This is simple. A process message is fed into the model, which returns a probability distribution. The argmax of the distribution is used to index a list of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreetingResponse\n",
      "CourtesyGreeting\n",
      "GoodBye\n"
     ]
    }
   ],
   "source": [
    "intents = enc.classes_\n",
    "test_greetresponse = 'my user is patrick'\n",
    "test_greet = 'hi, how are you'\n",
    "test_bye ='bye'\n",
    "\n",
    "def predict_intent(message):\n",
    "    processed_message = process_message(message)\n",
    "    pred_dist = model.predict(processed_message)\n",
    "    pred_idx = np.argmax(pred_dist)\n",
    "    pred_intent = intents[pred_idx]\n",
    "    return pred_intent\n",
    "\n",
    "print(predict_intent(test_greetresponse))\n",
    "print(predict_intent(test_greet))\n",
    "print(predict_intent(test_bye))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to take a predicted intent and have the bot respond to it. This generally consists of printing a random response associated with an intent to the screen. There are two exceptions. First, when a user gives the bot their user name, I would like the bot to respond using the name. Second, when a user says goodbye, it should end the conversation.\n",
    "\n",
    "The first is a named entity recognition task. When a user gives their name, I find the name using spaCy, so the bot can repeat it back. To address the second issue, I have the function return a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def predict_and_respond(message):\n",
    "    to_continue = True\n",
    "    ner_intents = ['GreetingResponse', 'CourtesyGreetingResponse']\n",
    "    quit_intents = ['GoodBye', 'CourtesyGoodBye']\n",
    "    pred_intent = predict_intent(message)\n",
    "\n",
    "    response = np.random.choice(response_lookup[pred_intent])\n",
    "\n",
    "    # Find entities, if necessary\n",
    "    if pred_intent in ner_intents:\n",
    "        doc = nlp(message)\n",
    "        entities = doc.ents\n",
    "        for ent in entities:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                user=ent.text\n",
    "        response = response.replace('<HUMAN>', user)\n",
    "    # Set to_continue to False if necessary\n",
    "    elif pred_intent in quit_intents:\n",
    "        to_continue = False\n",
    "\n",
    "    return response, to_continue\n",
    "\n",
    "def test_response(message):\n",
    "    '''\n",
    "    A function to test eh predictio and response function in a concise manner.\n",
    "    '''\n",
    "    print(f'Input message: {message}')\n",
    "    response, to_continue = predict_and_respond(message)\n",
    "    print(f'Predicted response: {response}')\n",
    "    print(f'Whether to continue chatting: {to_continue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input message: hi, how are you\n",
      "Predicted response: Hi, I am great, how are you? Please tell me your GeniSys user\n",
      "Whether to continue chatting: True\n",
      "Input message: my user is patrick\n",
      "Predicted response: Cool! Hello patrick, what can I do for you?\n",
      "Whether to continue chatting: True\n",
      "Input message: bye\n",
      "Predicted response: See you later\n",
      "Whether to continue chatting: False\n"
     ]
    }
   ],
   "source": [
    "test_response(test_greet)\n",
    "test_response(test_greetresponse)\n",
    "test_response(test_bye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I create the chatbot. This is simply a while loop. As long as to_continue is True, the user can enter text. If the user says goodbye, to_continue is set to False and the loop breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Hola human, please tell me your GeniSys user\n",
      "my user is patrick\n",
      "Good! Hi patrick, how can I help you?\n",
      "tell me a joke\n",
      "A woman goes to the doctor and says, 'Doctor, my husband limps because his left leg is an inch shorter than his right leg. What would you do in his case?' 'Probably limp, too', says the doc.\n",
      "bye\n",
      "Bye! Come back again soon.\n"
     ]
    }
   ],
   "source": [
    "def chat():\n",
    "    to_continue = True\n",
    "    while to_continue:\n",
    "        message = input('> ')\n",
    "        print(message)\n",
    "        response, cont = predict_and_respond(message)\n",
    "        \n",
    "        print(response)\n",
    "        to_continue = cont\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Noting the limitations discussed in the introduction, the bot has been successfully developed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54c3508af21ff7bd3bd04a3c70cfc8631b77736188a8a2e019278c5f75b5a6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
